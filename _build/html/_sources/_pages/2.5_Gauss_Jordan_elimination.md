(gauss-jordan-elimination-section)=

```{index} Gauss-Jordan elimination
```

```{index} Systems of linear equations ; solution using Gauss-Jordan elimination
```

# Gauss-Jordan elimination

```{figure} https://upload.wikimedia.org/wikipedia/commons/7/70/Wilhelm_Jordan.png
:width: 200px
:alt: Wilhelm Jordan
:figclass: margin

<a href="https://en.wikipedia.org/wiki/Wilhelm_Jordan_(geodesist)" target="_blank">Carl Wilhelm Jordan (1842 - 1899)</a>
```

**Gauss-Jordan elimination (GJE)**, named after <a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss" target="_blank">Gauss</a> and German geodesist <a href="https://en.wikipedia.org/wiki/Wilhelm_Jordan_(geodesist)" target="_blank">Wilhelm Jordan</a>, is similar to [Gaussian elimination](gaussian-elimination-section), but takes the process further - so that the values of the pivot elements are 1, and are the only non-zero element in their column. This allows the solution to be read from the final augmented matrix without the need to perform back-substitution. A matrix in this form is said to be in **reduced row echelon form**.

```{index} Reduced row echelon form
```

```{prf:definition} Reduced Row Echelon Form (RREF)
:label: rref-definition

A matrix is said to be in **Reduced Row Echelon Form (RREF)** if it satisfies the following conditions:

- It is in row echelon form
- The leading entry in each non-zero row has a value of 1
- The leading entry in each non-zero row is the only non-zero element in its column
```

For example, the following matrices are in reduced row echelon form:

$$ \begin{align*}
    &\begin{pmatrix}
        {\color{red}{1}} & 0 \\
        0 & {\color{red}{1}}
    \end{pmatrix}, &
    &\begin{pmatrix}
        {\color{red}{1}} & 2 & 0 \\
        0 & 0 & {\color{red}{1}}
    \end{pmatrix}, &
    &\begin{pmatrix}
        {\color{red}{1}} & 0 & 2 & 0 \\
        0 & {\color{red}{1}} & 3 & 0 \\
        0 & 0 & 0 & {\color{red}{1}}
    \end{pmatrix}
\end{align*} $$

The method of Gauss-Jordan elimination is summarised in {prf:ref}`gje-algorithm`.

```{index} Gauss-Jordan elimination ; algorithm
```

```{prf:algorithm} Gauss-Jordan elimination
:label: gje-algorithm

**Inputs:** An $m \times n$ coefficient matrix $A$ and an $m$-element constant vector $\mathbf{b}$.

**Outputs:** An augmented matrix in reduced row echelon form.

- Form the augmented matrix $( A \mid \mathbf{b} )$
- Initialise pivot row $k$ to 1
- For each column $j$ from 1 to $n$:
    - Perform a row swap of the pivot row if necessary (e.g. for partial pivoting, or if pivot element is zero)
    - If pivot element is zero, and all elements beneath the pivot element are zero, skip to the next column by incrementing $j$
- Divide row $k$ by pivot $a_{kj}$
- For each row $i$ from $1$ to $m$ where $i \neq k$:
    - Subtract $\dfrac{a_{ij}}{a_{jj}}$ times row $j$ from row $i$
- Increment $k$ to $k+1$ and repeat until you have done all the rows 
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/KMojbHRNY9o?si=54PqTE2f3Xl_SpQ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

```{prf:example}
:label: gje-example

Use Gauss-Jordan elimination to solve the following system of linear equations

$$ \begin{align*}
    3x_1 + x_2 - 2 x_3 &= 1, \\
    x_1 - x_2 + 2x_3 &= 3, \\
    2x_1 - 3x_2 + 7x_3 &= 4.
\end{align*} $$

```{dropdown} Solution

Row reduce the augmented matrix to reduced row echelon form

$$ \begin{align*}
    & \left( \begin{array}{ccc|c}
        3 & 1 & -2 & 1 \\
        1 & -1 & 2 & 3 \\
        2 & -3 & 7 & 4
    \end{array} \right)
    \\ \\
     R_1 \leftrightarrow R_2 \longrightarrow
    & \left( \begin{array}{ccc|c}
        1 & -1 & 2 & 3 \\
        3 & 1 & -2 & 1 \\
        2 & -3 & 7 & 4
    \end{array} \right)
    \\ \\
    \begin{array}{l} \\ R_2 - 3R_1 \\ R_3 - 2R_1 \end{array}  \longrightarrow
    & \left( \begin{array}{ccc|c}
        1 & -1 & 2 & 3 \\
        0 & 4 & -8 & -8 \\
        0 & -1 & 3 & -2
    \end{array} \right)
    \\ \\
    \frac{1}{4} R_2 \longrightarrow
    & \left( \begin{array}{ccc|c}
        1 & -1 & 2 & 3 \\
        0 & 1 & -2 & -2 \\
        0 & -1 & 3 & -2
    \end{array} \right)
 	\\ \\
    \begin{array}{l} R_1 + R_2 \\ \\ R_3 + R_2 \end{array} \longrightarrow
    & \left( \begin{array}{ccc|c}
        1 & 0 & 0 & 1 \\
        0 & 1 & -2 & -2 \\
        0 & 0 & 1 & -4
    \end{array} \right)
    \\ \\
     R_2 + 2R_3 \longrightarrow
    & \left( \begin{array}{ccc|c}
        1 & 0 & 0 & 1 \\
        0 & 1 & 0 & -10 \\
        0 & 0 & 1 & -4
    \end{array} \right)
\end{align*} $$

Therefore the solution is $x_1 = 1$, $x_2 = -10$ and $x_3 = -4$.
```

For any given matrix, its reduced row echelon form is unique. If the matrix is square ($n \times n$) and non-singular (invertible), the reduced row echelon form of the matrix will be $I_n$, the $n \times n$ identity matrix.

    
---

```{index} Elementary matrices
```

## Elementary matrices

Gauss-Jordan elimination allows us to calculate the inverse of matrices, and is much more computationally efficient than the {prf:ref}`adjoint-determinant formula<adjoint-definition>`. To show how we can use row operations to calculate the inverse of a matrix, we first need to consider elementary matrices.

```{prf:definition} Elementary matrices
:label: elementary-matrices-definition

An **elementary matrix** is an $n \times n$ square matrix which can be obtained by performing a single elementary row operation on the identity matrix $I_n$.
```

Since we have {prf:ref}`three types<ero-definition>` of elementary row operations, there are three corresponding types of elementary matrices. Consider examples of each of the three types for $I_3$, the $3 \times 3$ identity matrix:

- Swap row 1 and row 2:
  
$$ \begin{align*}
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
    \qquad R_1 \leftrightarrow R_2 \longrightarrow
    \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}
\end{align*} $$

- Multiply row 2 by $k$:

$$ \begin{align*}
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
    \qquad kR_2 \longrightarrow
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & k & 0 \\ 0 & 0 & 1 \end{pmatrix}
\end{align*} $$

- Add $k$ multiples of row 1 to row 3:

$$ \begin{align*}
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
    \qquad R_3 + kR_1 \longrightarrow
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ k & 0 & 1 \end{pmatrix}
\end{align*} $$

Elementary matrices have an inverse, which is obtained by **inverting the elementary row operation** and applying it to the identity matrix. The operations used above have the following inverse operations :

- Swapping row 1 and row 2 can be inverted by simply performing the same swap again:
  
$$ \begin{align*}
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
    \qquad R_1 \leftrightarrow R_2 \longrightarrow
    \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}
\end{align*} $$

- Multiplying row 2 by $k$ can be inverted by dividing row 2 by $k$:

$$ \begin{align*}
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
    \qquad \frac{1}{k} R_2  \longrightarrow
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{k} & 0 \\ 0 & 0 & 1 \end{pmatrix}
\end{align*} $$

- Adding $k$ multiples of row 1 to row 3 can be inverted by subtracting $k$ multiples of row 1 from row 3:

$$ \begin{align*}
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
    \qquad R_3 - kR_1 \longrightarrow
    \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -k & 0 & 1 \end{pmatrix}
\end{align*} $$

```{prf:theorem} Multiplying by an elementary matrix
:label: elementary-matrix-multiplication-theorem

If $A$ is an $n \times n$ matrix and $E$ is an elementary matrix, the product $EA$ is equivalent to performing the elementary row operation used to obtain $E$ on $A$.
```

For example, let $A = \begin{pmatrix} 1 & 0 & 4 \\ 2 & -1 & 3 \\ 0 & 5 & 1 \end{pmatrix}$ and consider the following row operations:

- $R_1 \leftrightarrow R_2$: &emsp; The corresponding elementrary matrix is  $E_1 = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}$ so

$$E_1A = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 & 4 \\ 2 & -1 & 3 \\ 0 & 5 & 1 \end{pmatrix} =  \begin{pmatrix} 2 & -1 & 3 \\ 1 & 0 & 4 \\ 0 & 5 & 1 \end{pmatrix}$$
  
- $-2R_2$: &emsp; The elementrary matrix is $E_2 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & 1 \end{pmatrix}$ so

$$E_2A = \begin{pmatrix} 1 & 0 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 & 4 \\ 2 & -1 & 3 \\ 0 & 5 & 1 \end{pmatrix}=\begin{pmatrix} 1 & 0 & 4 \\ -4 & 2 & -6 \\ 0 & 5 & 1 \end{pmatrix}$$
  
- $R_3 + 3R_2$: &emsp; The elementrary matrix is  $E_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 3 & 1 \end{pmatrix}$ so

$$E_3A = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 3 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 & 4 \\ 2 & -1 & 3 \\ 0 & 5 & 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 & 4 \\ 2 & -1 & 3 \\ 6 & 2 & 10 \end{pmatrix}$$

---

## Calculating the inverse of a matrix using Gauss-Jordan elimination

```{index} Matrix ; inverse using Gauss-Jordan elimination
```

```{prf:theorem}
If $A$ is a non-singular (invertible) square matrix, then the inverse of $A$ can be calculated as:

$$A^{-1} = E_kE_{k-1}\cdots E_2E_1$$

Where $E_1, E_2, \ldots, E_k$ are the elementary matrices corresponding to the $k$ elementary row operations needed to reduce $A$ to reduced row echelon form.
```

```{prf:proof}
Since $A$ is square and non-singular, its reduced row echelon form will be the identity matrix. If we can apply $k$ elementary row operations with corresponding elementary matrices $E_1, E_2, \ldots, E_k$ to row reduce $A$ to reduced row echelon form, then we have

$$E_k E_{k-1} \cdots E_2 E_1 A = I$$

That is, if we multiply $A$ on the left by all of the elementary matrices in turn, we will get the identity matrix.  Multiplying $A^{-1}$ to the right of both sides gives

$$ \begin{align*}
 E_k E_{k-1} \cdots E_2 E_1 A A^{-1} &= I A^{-1} \\
 E_k E_{k-1} \cdots E_2 E_1 &= A^{-1}
\end{align*} $$
as required.
```

So we can calculate the inverse of $A$ by applying the same row operations to the identity matrix as we do when we reduce $A$ to reduced row echelon form. In practice, we can do this by forming an augmented matrix $(A \mid I)$ and performing Gauss-Jordan elimination on the augmented matrix. Once this has been converted to reduced row echelon form, the matrix to the right of the partition will then be the inverse of $A$.

The calculation of a matrix inverse using Gauss-Jordan elimination is more efficient than using the adjoint-determinant formula when dealing with larger matrices (i.e., when $n > 3$) since the steps can be easily programmed into a computer, and it does not require the calculation of determinants - which can be computationally expensive.

<iframe width="560" height="315" src="https://www.youtube.com/embed/WKz40mnevwE?si=xLZZRR2JT9TeQa8m" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

```{prf:example}
:label: gauss-jordan-inverse-example

Use Gauss-Jordan elimination to calculate the inverse of

$$ A =
\begin{pmatrix}
  1 & 0 & 2 \\
  2 & -1 & 3 \\
  1 & 4 & 4
\end{pmatrix}.$$

```{dropdown} Solution

Row reduce the augmented matrix $(A \mid I)$ to reduced row echelon form

$$ \begin{align*}
    &\left( \begin{array}{ccc|ccc}
      1 & 0 & 2 & 1 & 0 & 0 \\
      2 & -1 & 3 & 0 & 1 & 0 \\
      1 & 4 & 4 & 0 & 0 & 1
    \end{array} \right)
     \\ \\
    \begin{array}{l} R_2 - 2R_1 \\ R_3 - R_1 \end{array} \longrightarrow
    &\left( \begin{array}{ccc|ccc}
      1 & 0 & 2 & 1 & 0 & 0 \\
      0 & -1 & -1 & -2 & 1 & 0 \\
      0 & 4 & 2 & -1 & 0 & 1
    \end{array} \right)
    \\ \\
    -R_2\longrightarrow
    &\left( \begin{array}{ccc|ccc}
      1 & 0 & 2 & 1 & 0 & 0 \\
      0 & 1 & 1 & 2 & -1 & 0 \\
      0 & 4 & 2 & -1 & 0 & 1
    \end{array} \right)
    \\ \\
    R_3 - 4R_2 \longrightarrow
    &\left( \begin{array}{ccc|ccc}
      1 & 0 & 2 & 1 & 0 & 0 \\
      0 & 1 & 1 & 2 & -1 & 0 \\
      0 & 0 & -2 & -9 & 4 & 1
    \end{array} \right)
    \\ \\
    -\frac{1}{2}R_3 \longrightarrow
    &\left( \begin{array}{ccc|ccc}
      1 & 0 & 2 & 1 & 0 & 0 \\
      0 & 1 & 1 & 2 & -1 & 0 \\
      0 & 0 & 1 & \frac{9}{2} & -2 & -\frac{1}{2}
    \end{array} \right)
    \\ \\
    \begin{array}{l} R_1 - 2R_3 \\ R_2 - R_3 \end{array}  \longrightarrow
    &\left( \begin{array}{ccc|ccc}
      1 & 0 & 0 & -8 & 4 & 1 \\
      0 & 1 & 1 & -\frac{5}{2} & 1 & \frac{1}{2} \\
      0 & 0 & 1 & \frac{9}{2} & -2 & -\frac{1}{2}
    \end{array} \right)  
\end{align*} $$

The augmented matrix is now in reduced row echelon form and the inverse of $A$ is

$$ A^{-1} =
\begin{pmatrix} -8 & 4 & 1 \\ -\frac{5}{2} & 1 & \frac{1}{2} \\ \frac{9}{2} & -2 & -\frac{1}{2} \end{pmatrix}. $$

Checking whether this is correct

$$ \begin{align*}
  A^{-1} A &=
  \begin{pmatrix}
    -8 & 4 & 1 \\
    -\frac{5}{2} & 1 & \frac{1}{2} \\
    \frac{9}{2} & -2 & -\frac{1}{2} \end{pmatrix}
  \end{align*}
  \begin{pmatrix}
    1 & 0 & 2 \\
    2 & -1 & 3 \\
    1 & 4 & 4
  \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{pmatrix} \checkmark$$
```
