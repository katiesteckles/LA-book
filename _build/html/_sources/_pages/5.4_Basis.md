(basis-section)=

```{index} Basis
```

# Basis

We have already met the concept of a basis in the chapter on vectors, where we saw that [the basis vectors for $\mathbb{R}^3$](basis-vectors-section) are the vectors $\mathbf{i} = (1, 0, 0)$, $\mathbf{j} = (0, 1, 0)$ and $\mathbf{k} = (0, 0, 1)$ and any vector in $\mathbb{R}^3$ can be represented as a linear combination of these basis vectors.

The concept of a basis is not limited to Euclidean geometry, and we can define a basis for any vector space, so that every element in the vector space can be expressed as a {prf:ref}`linear combination<linear-combination-of-vectors-definition>` of the basis elements.

---

```{index} Spanning set
```

## Spanning sets

```{prf:definition} Spanning set
:label: spanning-set-definition

Let $V$ be a vector space over the field $F$, and let $S$ be a subset of $V$. We define $W$ to be the subset of $V$ containing all vectors that are expressible as a linear combination of vectors in $S$ - that is, for any $u \in W$, given the vectors $v_1, \ldots, v_n \in S$ we can find $\alpha_1, \ldots, \alpha_n \in F$ so that:

$$ u = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n, $$

Then $W$ is a subspace of $V$, and $S$ is a **spanning set** for $W$. We call $W$ the **span** of the set $S$, and write this as $W = \operatorname{span}(S)$.
```

For example, to show that $\{ \mathbf{v}_1, \mathbf{v}_2 \}$ is a spanning set for $\mathbb{R}^2$, where $\mathbf{v}_1 = (2, 1)$ and $\mathbf{v}_2 = (4, 3)$, we need to show that

$$\alpha_1 \begin{pmatrix} 2\\ 1 \end{pmatrix} + \alpha_2 \begin{pmatrix} 4 \\ 3 \end{pmatrix} = \begin{pmatrix} a\\ b \end{pmatrix},$$

for any $a, b \in \mathbb{R}$, i.e., that the following system has a non-trivial solution:

$$ \begin{align*}
    2 \alpha_1 + 4 \alpha_2 &= a, \\
    \alpha_1 + 3 \alpha_2 &= b.
\end{align*} $$

A system of linear equations has a solution if it is {prf:ref}`non-singular<inverse-matrix-definition>` (the determinant of the coefficient matrix is non-zero), therefore

$$ \det \begin{pmatrix} 2 & 4 \\ 1 & 3 \end{pmatrix} = 2 \neq 0, $$

so there is a solution to the system and $\{\mathbf{v}_1, \mathbf{v}_2\}$ is a spanning set for $\mathbb{R}^2$.

---

## Basis of a vector space

```{prf:definition} Basis of a vector space
:label: basis-definition

A **basis** of a vector space $V$ over a field $F$ is a linearly independent subset of $V$ that spans $V$. A subset $B$ is a basis if it satisfies the following:

- {prf:ref}`linear independence property<linear-dependence-definition>`: if $B = \{b_1, \ldots , b_n\}$, the following equation only has the trivial solution $\alpha_i = 0$:

$$ \alpha_1 b_1 + \alpha_2 b_2 + \cdots + \alpha_n b_n = 0; $$

- {prf:ref}`spanning property<spanning-set-definition>`: for all $u \in V$ we can write $u$ as a linear combination of $v \in B$, i.e.,

$$ u = \alpha_1 b_1 + \alpha_2 b_2 + \cdots + \alpha_n b_n. $$
```

```{index} Basis ; orthogonal basis
```

```{prf:definition} Orthogonal basis
:label: orthogonal-basis-definition

An **orthogonal basis** of a vector space is one in which each of the vectors are orthogonal (perpendicular) to one another.
```

```{index} Basis ; orthonormal basis
```

```{prf:definition} Orthonormal basis
:label: orthonormal-basis-definition

An **orthonormal basis** of a vector space is one in which each of the vectors are orthogonal to one another and each vector is a unit vector.
```

```{index} Vector space ; dimension
```

```{prf:definition} Dimension of a vector space
:label: vector-space-dimension-definition

The **dimension** of a vector space $V$ is denoted by $\dim(V)$ and is the number of elements in a basis for the vector space.
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/HQBVfacuPOU?si=Sy0YBo-PBfDYaAK4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

```{prf:example}
:label: basis-example

Show that $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\}$, where $\mathbf{v}_1 = (1, 1, 0)$, $\mathbf{v}_2 = (1, -1, 1)$ and $\mathbf{v}_3 = (1, -1, -2)$, is a basis for $\mathbb{R}^3$.

```{dropdown} Solution

We need to show that the three vectors are linearly independent, i.e., show that the only solution to the following system is $\alpha_1 = \alpha_2 = \alpha_3 = 0$

$$ \begin{align*}
    \alpha_1 \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} +
    \alpha_2 \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} +
    \alpha_3 \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix}  =
    \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\end{align*} $$

Using Gauss-Jordan elimination

$$ \begin{align*}
    & \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & 1 \\
        1 & -1 & -2
    \end{pmatrix}
     \\ \\
    \begin{matrix} R_2 - R_1 \\ R_3 - R_1 \end{matrix}\longrightarrow &
    \begin{pmatrix}
        1 & 1 & 1 \\
        0 & -2 & 0 \\
        0 & -2 & -3
    \end{pmatrix}
	\\ \\
    -\frac{1}{2} R_2\longrightarrow &
    \begin{pmatrix}
        1 & 1 & 1 \\
        0 & 1 & 0 \\
        0 & -2 & -3
    \end{pmatrix}
     \\\\
    \begin{matrix} R_1 - R_2 \\ R_3 + 2R_2 \end{matrix} \longrightarrow &
    \begin{pmatrix}
        1 & 0 & 1 \\
        0 & 1 & 0 \\
        0 & 0 & -3
    \end{pmatrix}
     \\ \\
    -\frac{1}{3}R_3 \longrightarrow &
    \begin{pmatrix}
        1 & 0 & 1 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \\ \\
    R_1 - R_3\longrightarrow &
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
\end{align*} $$

So $\mathbf{v}_1$, $\mathbf{v}_2$ and $\mathbf{v}_3$ are linearly independent. We also need to show that $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ spans $\mathbb{R}^3$.

$$ \det \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & -1 \\ 0 & 1 & -2 \end{pmatrix} = 3 + 3 = 6 \neq 0, $$

so $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ is a spanning set for $\mathbb{R}^3$. Note that in showing that $\mathbf{v}_1$, $\mathbf{v}_2$ and $\mathbf{v}_3$ are linearly independent we have also shown that $\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \alpha_3 \mathbf{v}_3 = (a, b, c)$ has a solution and $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ is a spanning set. We can also check if this is an orthogonal basis:

$$ \begin{align*}
    \mathbf{v}_1 \cdot \mathbf{v}_2 &= \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} = 0, \\
    \mathbf{v}_1 \cdot \mathbf{v}_3 &= \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ 2 \end{pmatrix} = 0, \\
    \mathbf{v}_2 \cdot \mathbf{v}_3 &= \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix} = 0,
\end{align*} $$

so $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ is an orthogonal basis, since each pair of vectors in it is perpendicular. It is not an orthonormal basis (all vectors of unit length) since $\|\mathbf{v}_1\| = \sqrt{2} \neq 1$.
```

---

```{index} Basis ; change of basis
```

```{index} Basis ; standard basis
```

(change-of-basis-section)=
## Change of basis

$\mathbb{R}^n$ has a particularly nice basis that is easy to write down:

$$ E = \left\{
        \mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix},
        \mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \ldots,
        \mathbf{e}_n = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix}
        \right\}, $$

which is called the **standard basis**. Note that $\mathbf{e}_i$ is column $i$ of the identity matrix, and for $\mathbb{R}^3$ we have $\mathbf{i} = \mathbf{e}_1$, $\mathbf{j} = \mathbf{e}_2$ and $\mathbf{k} = \mathbf{e}_3$.

We can represent a vector $\mathbf{u} = (u_1, u_2, \ldots, u_n) \in \mathbb{R}^n$ as a sum of elements of the standard basis, since it will just be a sum of the basis vectors using the entries of $\mathbf{u}$ as coefficients:

$$\mathbf{u} = u_1 \mathbf{e}_1 + u_2 \mathbf{e}_2 + \cdots + u_n \mathbf{e}_n $$

We can also express the same vector $\mathbf{u}$ as a sum of the elements of another basis $W = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$. We denote this $[\mathbf{u}]_W$, and in order to express $\mathbf{u}$ with respect to the basis $W$ we need to solve:

$$ \mathbf{u} = w_1 \mathbf{v}_1 + w_2 \mathbf{v}_2 + \cdots + w_n \mathbf{v}_n$$

for $w_1, w_2, \ldots, w_n$, using methods we have already seen.

A visual representation of a single vector being expressed in terms of two different bases is shown in {numref}`change-of-basis-figure`.

```{figure} /_images/5_change_of_basis.svg
:name: change-of-basis-figure
:width: 400

Representing the same point with respect to two different basis.
```

We often think of the coordinates of a point as being equivalent to a vector (from the origin to that point). Since we have different sets of basis vectors, we can use them to give the coordinates to a point but with respect to a different basis. 

We would say, the point whose co-ordinates are $(u_1, u_2)$ with respect to the standard basis $\{ \mathbf{e}_1, \mathbf{e}_2\}$ can also be defined using the co-ordinates $(w_1, w_2)$, with respect to the basis $\{ \mathbf{w}_1, \mathbf{w}_2 \}$.

<iframe width="560" height="315" src="https://www.youtube.com/embed/HQBVfacuPOU?si=OZW5J0qBTpuMi1U2&amp;start=126" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

```{prf:example}
:label: change-of-basis-example

Represent the vector $\mathbf{u} = (4, 0, 5)$ with respect to the basis $W = \{ (1, 1, 0), (1, -1, 1), (1, -1, -2)\}$.

```{dropdown} Solution

We need to solve the system

$$ w_1 \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} +
    w_2 \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} +
    w_3 \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix} =
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix}, $$

which can be written as the matrix equation

$$ \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & -1 \\
        0 & 1 & -2
    \end{pmatrix}
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} =
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix}, $$

Calculating the inverse of the coefficient matrix

$$ \begin{align*}
    A^{-1} &=
    \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & -1 \\
        0 & 1 & -2
    \end{pmatrix}^{-1}
    =
    \frac{1}{6}
    \begin{pmatrix}
        3 & 2 & 1 \\
        3 & -2 & -1 \\
        0 & 2 & -2
    \end{pmatrix}^\mathsf{T}
    =
    \begin{pmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        \frac{1}{3} & -\frac{1}{3} & \frac{1}{3} \\
        \frac{1}{6} & -\frac{1}{6} & -\frac{1}{3}
    \end{pmatrix},
\end{align*} $$

so

$$ \begin{align*}
    [\mathbf{u}]_W =
    \begin{pmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        \frac{1}{3} & -\frac{1}{3} & \frac{1}{3} \\
        \frac{1}{6} & -\frac{1}{6} & -\frac{1}{3}
    \end{pmatrix}
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix} =
    \begin{pmatrix} 2 \\ 3 \\ -1 \end{pmatrix}.
\end{align*} $$
```

In {prf:ref}`change-of-basis-example` we calculated an inverse matrix (shown in the final line of the solution), which we used to find the vector $\mathbf{u}$ expressed in terms of the new basis.

In general, once we have calculated this matrix, we can use this to find any vector $\mathbf{u}$ with respect to the basis $W$. This matrix is known as the <a href="https://en.wikipedia.org/wiki/5_change_of_basis" target="_blank">**change of basis matrix**</a>.

```{index} Basis ; change of basis matrix
```

```{prf:definition} Change of basis matrix
:label: change-of-basis-matrix-definition

Let $V$ be a vector space over the field $F$ and $\mathbf{u} \in V$. If $E$ and $W$ are two bases for $V$, then the change of basis matrix is the matrix $A_{E \to W}$ such that $[u]_{W} = A_{E \to W} [u]_E$.
```

To express the vector $\mathbf{u} \in \mathbb{R}^3$ with respect to the basis $W$, we multiply $\mathbf{u}$ by the change of basis matrix $A_{E \to W}$.

Going from the standard basis to a non-standard basis, the calculations to find this matrix are relatively straightforward; changing between two non-standard bases is a slightly more complicated procedure, and will be covered in the more advanced units on linear algebra.
