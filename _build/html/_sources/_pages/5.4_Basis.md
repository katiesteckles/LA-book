(basis-section)=

```{index} Basis
```

# Basis

We have already met the concept of a basis in the chapter on vectors where we saw that [the basis vectors for $\mathbb{R}^3$](basis-vectors-section) are the vectors $\mathbf{i} = (1, 0, 0)$, $\mathbf{j} = (0, 1, 0)$ and $\mathbf{k} = (0, 0, 1)$ and any vector in $\mathbb{R}^3$ can be represented as a linear combination of these basis vectors. The concept of a basis is not limited to Euclidean geometry, and we can define a basis for any vector space so that an element in the vector space can be expressed as a {prf:ref}`linear combination<linear-combination-of-vectors-definition>` of the basis elements.

---

```{index} Spanning set
```

## Spanning sets

```{prf:definition} Spanning set
:label: spanning-set-definition

Let $V$ be a vector space over the field $F$ and $S$ is a subset of $V$. Let $W$ be a subset of $V$ that are expressible as a linear combination of vectors in $S$, i.e., for $u \in W$, $v_1, \ldots, v_n \in S$ and $\alpha_1, \ldots, \alpha_n \in F$

$$ u = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n, $$

then $W$ is a subspace of $V$ and $S$ is a **spanning set** for $W$. We write this as $\operatorname{span}(S)$.
```

For example, to show that $\{ \mathbf{v}_1, \mathbf{v}_2 \}$ where $\mathbf{v}_1 = (2, 1)$ and $\mathbf{v}_2 = (4, 3)$ is a spanning set for $\mathbb{R}^2$ we need to show that

$$\alpha_1 \begin{pmatrix} 2\\ 1 \end{pmatrix} + \alpha_2 \begin{pmatrix} 4 \\ 3 \end{pmatrix} = \begin{pmatrix} a\\ b \end{pmatrix},$$

for any $a, b \in \mathbb{R}$, i.e., that the following system has a non-trivial solution

$$ \begin{align*}
    2 \alpha_1 + 4 \alpha_2 &= a, \\
    \alpha_1 + 3 \alpha_2 &= b.
\end{align*} $$

A system of linear equations has a solution if it is {prf:ref}`non-singular<inverse-matrix-definition>` (the determinant of the coefficient matrix is non-zero), therefore

$$ \det \begin{pmatrix} 2 & 4 \\ 1 & 3 \end{pmatrix} = 2 \neq 0, $$

so there is a solution to the system and $\{\mathbf{v}_1, \mathbf{v}_2\}$ is a spanning set for $\mathbb{R}^2$.

---

## Basis of a vector space

```{prf:definition} Basis of a vector space
:label: basis-definition

A **basis** of a vector space $V$ of a field $F$ is a linearly independent subset of $V$ that spans $V$. A subset $W$ is a basis if it satisfies the following:

- {prf:ref}`linear independence property<linear-dependence-definition>`: for every subset $\{v_1, \ldots , v_n\}$ of $W$ the following equation only has the trivial solution $\alpha_i = 0$.

$$ \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = 0; $$

- {prf:ref}`spanning property<spanning-set-definition>`: for all $u \in V$ we can write $u$ as a linear combination of $v \in W$, i.e.,

$$ u = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n. $$
```

```{index} Basis ; orthogonal basis
```

```{prf:definition} Orthogonal basis
:label: orthogonal-basis-definition

An **orthogonal basis** of a vector space is one in which each of the vectors are orthogonal (perpendicular) to one another.
```

```{index} Basis ; orthonormal basis
```

```{prf:definition} Orthonormal basis
:label: orthonormal-basis-definition

An **orthonormal basis** of a vector space is one in which each of the vectors are orthogonal to one another and each vector is a unit vector.
```

```{index} Vector space ; dimension
```

```{prf:definition} Dimension of a vector space
:label: vector-space-dimension-definition

The **dimension** of a vector space $V$ is denoted by $\dim(V)$ and is the number of elements in the basis for the vector space.
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/HQBVfacuPOU?si=Sy0YBo-PBfDYaAK4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

```{prf:example}
:label: basis-example

Show that $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\}$ where $\mathbf{v}_1 = (1, 1, 0)$, $\mathbf{v}_2 = (1, -1, 1)$ and $\mathbf{v}_3 = (1, -1, -2)$ is a basis for $\mathbb{R}^3$.

```{dropdown} Solution

We need to show that the three vectors are linearly independent, i.e., show that the only solution to the following system is $\alpha_1 = \alpha_2 = \alpha_3 = 0$

$$ \begin{align*}
    \alpha_1 \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} +
    \alpha_2 \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} +
    \alpha_3 \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix}  =
    \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\end{align*} $$

Using Gauss-Jordan elimination

$$ \begin{align*}
    & \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & 1 \\
        1 & -1 & -2
    \end{pmatrix}
    \begin{matrix} \\ R_2 - R_1 \\ R_3 - R_1 \end{matrix} &
    \longrightarrow &
    \begin{pmatrix}
        1 & 1 & 1 \\
        0 & -2 & 0 \\
        0 & -2 & -3
    \end{pmatrix}
    \begin{matrix} \\ -\frac{1}{2}R_2 \\ \phantom{x} \end{matrix} \\ \\
    \longrightarrow &
    \begin{pmatrix}
        1 & 1 & 1 \\
        0 & 1 & 0 \\
        0 & -2 & -3
    \end{pmatrix}
    \begin{matrix} R_1 - R_2 \\ \\ R_3 + 2R_2 \end{matrix} &
    \longrightarrow &
    \begin{pmatrix}
        1 & 0 & 1 \\
        0 & 1 & 0 \\
        0 & 0 & -3
    \end{pmatrix}
    \begin{matrix} \\ \phantom{x} \\ -\frac{1}{3}R_3 \end{matrix} \\ \\
    \longrightarrow &
    \begin{pmatrix}
        1 & 0 & 1 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{matrix} R_1 - R_3 \\ \phantom{x} \\ \phantom{x} \end{matrix} &
    \longrightarrow &
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
\end{align*} $$

So $\mathbf{v}_1$, $\mathbf{v}_2$ and $\mathbf{v}_3$ are linearly independent. We also need to show that $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ spans $\mathbb{R}^3$.

$$ \det \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & -1 \\ 0 & 1 & -2 \end{pmatrix} = 3 + 3 = 6 \neq 0, $$

so $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ is a spanning set for $\mathbb{R}^3$. Note that in showing that $\mathbf{v}_1$, $\mathbf{v}_2$ and $\mathbf{v}_3$ are linearly independent we have also shown that $\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \alpha_3 \mathbf{v}_3 = (a, b, c)$ has a solution and $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ is a spanning set. Furthermore

$$ \begin{align*}
    \mathbf{v}_1 \cdot \mathbf{v}_2 &= \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} = 0, \\
    \mathbf{v}_1 \cdot \mathbf{v}_3 &= \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ 2 \end{pmatrix} = 0, \\
    \mathbf{v}_2 \cdot \mathbf{v}_3 &= \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix} = 0,
\end{align*} $$

so $\{ \mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \}$ is an orthogonal basis. It is not an orthonormal basis since $\|\mathbf{v}_1\| = \sqrt{2} \neq 1$.
```

---

```{index} Basis ; change of basis
```

```{index} Basis ; standard basis
```

(change-of-basis-section)=
## Change of basis

$\mathbb{R}^n$ has a particularly nice basis that is easy to write down

$$ E = \left\{
        \mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix},
        \mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \ldots,
        \mathbf{e}_n = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix}
        \right\}, $$

which is called the **standard basis**. Note that $\mathbf{e}_i$ is column $i$ of the identity matrix and for $\mathbb{R}^3$ we have $\mathbf{i} = \mathbf{e}_1$, $\mathbf{j} = \mathbf{e}_2$ and $\mathbf{k} = \mathbf{e}_3$.

We can represent a vector $\mathbf{u} = (u_1, u_2, \ldots, u_n) \in \mathbb{R}^n$ in the standard basis as a vector with respect to another basis $W = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$ which is denoted using $[\mathbf{u}]_W$. Using the standard basis we have

$$u_1 \mathbf{e}_1 + u_2 \mathbf{e}_2 + \cdots + u_n \mathbf{e}_n = \mathbf{u}, $$

and to express $\mathbf{u}$ with respect to the basis $W$ we need to solve

$$  w_1 \mathbf{v}_1 + w_2 \mathbf{v}_2 + \cdots + w_n \mathbf{v}_n = \mathbf{u}, $$

for $w_1, w_2, \ldots, w_n$. This concept is illustrated for $\mathbb{R}^2$ in {numref}`change-of-basis-figure`.

```{figure} /_images/5_change_of_basis.svg
:name: change-of-basis-figure
:width: 400

Representing the same point with respect to two different basis.
```

The point with co-ordinates $(u_1, u_2)$ with respect to the standard basis $\{ \mathbf{e}_1, \mathbf{e}_2\}$ can be expressed with respect to the basis $\{ \mathbf{w}_1, \mathbf{w}_2 \}$ by the co-ordinates $(w_1, w_2)$.

<iframe width="560" height="315" src="https://www.youtube.com/embed/HQBVfacuPOU?si=OZW5J0qBTpuMi1U2&amp;start=126" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

```{prf:example}
:label: change-of-basis-example

Represent the vector $\mathbf{u} = (4, 0, 5)$ with respect to the basis $W = \{ (1, 1, 0), (1, -1, 1), (1, -1, -2)\}$.

```{dropdown} Solution

We need to solve the system

$$ w_1 \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} +
    w_2 \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} +
    w_3 \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix} =
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix}, $$

which can be written as the matrix equation

$$ \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & -1 \\
        0 & 1 & -2
    \end{pmatrix}
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} =
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix}, $$

Calculating the inverse of the coefficient matrix

$$ \begin{align*}
    A^{-1} &=
    \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & -1 \\
        0 & 1 & -2
    \end{pmatrix}^{-1}
    =
    \frac{1}{6}
    \begin{pmatrix}
        3 & 2 & 1 \\
        3 & -2 & -1 \\
        0 & 2 & -2
    \end{pmatrix}^\mathsf{T}
    =
    \begin{pmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        \frac{1}{3} & -\frac{1}{3} & \frac{1}{3} \\
        \frac{1}{6} & -\frac{1}{6} & -\frac{1}{3}
    \end{pmatrix},
\end{align*} $$

so

$$ \begin{align*}
    [\mathbf{u}]_W =
    \begin{pmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        \frac{1}{3} & -\frac{1}{3} & \frac{1}{3} \\
        \frac{1}{6} & -\frac{1}{6} & -\frac{1}{3}
    \end{pmatrix}
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix} =
    \begin{pmatrix} 2 \\ 3 \\ -1 \end{pmatrix}.
\end{align*} $$
```

In {prf:ref}`change-of-basis-example` we can represent any vector $\mathbf{u}$ with respect to the basis $W$ by multiplying by the square matrix in the final equation. This matrix is known as the <a href="https://en.wikipedia.org/wiki/5_change_of_basis" target="_blank">**change of basis matrix**</a>.

```{index} Basis ; change of basis matrix
```

```{prf:definition} Change of basis matrix
:label: change-of-basis-matrix-definition

Let $V$ be a vector space over the field $F$ and $u \in V$. If $E$ and $W$ are two basis for $V$ then the change of basis matrix is the matrix $A_{E \to W}$ such that $[u]_{W} = A_{E \to W} [u]_E$.
```

So to express the vector $\mathbf{u} \in \mathbb{R}^3$ with respect to the basis $W$ we simply multiply $\mathbf{u}$ by the change of basis matrix. Changing from a non-standard basis is a slightly more complicated procedure and will be covered in the more advanced units on linear algebra.
