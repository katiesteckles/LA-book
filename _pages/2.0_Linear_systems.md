```{index} Linear systems
```

(systems-of-linear-equations-chapter)=

# Systems of Linear Equations

```{index} Systems of linear equations
```

Systems of linear equations, and the methods used to solve them, are a fundamental part of linear algebra. Systems of equations can be used to describe relationships between sets of variables, and have many applications including co-ordinate geometry, numerical methods, computer science, engineering and statistics. There are many techniques we can use to compute the solutions to systems of linear equations, and we will be covering the most common of these.

A linear equation is an equation that involves a set of **variables** multiplied by scalar **coefficients**, where the sum of these is equal to some **constant** scalar value. Generally, if $x_1, x_2, \ldots, x_n$ are variables, $a_1, a_2, \ldots, a_n$ are coefficients and $b$ is a constant value then a linear equation can take the form

$$ a_1 x_1 + a_2 x_2 + \cdots + a_n x_n = b. $$

Examples of linear equations:

$$ 3x + 2y = 5 \qquad x = 2 \qquad x + y + z = 2 $$

A **nonlinear equation** is an equation where one or more of the variables appear with exponents not equal to 1 (e.g. a polynomial equation like $x^2 = 4$), or two or more of the variables are multiplied together (e.g. an equation in which $x$ and $y$ are variables, that contains a term like $xy$). The field of linear algebra is only concerned with linear equations.

If we were to plot all the points that satisfy a linear equation, we would see a straight line. (Nonlinear equations generally define curved lines.) The solutions to systems of linear equations are points which satisfy all the equations at once, which notionally correspond to points where the lines intersect in $n$-dimensional space, where $n$ is the number of variables in the system. 

```{prf:definition} System of linear equations
:label: system-of-linear-equation-definition

A **system of linear equations** is a collection of one or more linear equations expressed using the same set of variables. For example,

$$ \begin{align*}
    a_{11} x_1+a_{12} x_2+\cdots +a_{1n}x_n &=b_1, \\
    a_{21} x_1+a_{22} x_2+\cdots+a_{2n}x_n &=b_2, \\
    &\vdots \\
    a_{m1} x_1+a_{m2} x_2+\cdots+a_{mn}x_n &=b_m,
\end{align*} $$

where $x_1, x_2, \ldots, x_n$ are **variables**, $a_{11}, a_{12}, \ldots, a_{mn}$ are **coefficients** and $b_1, b_2, \ldots, b_n$ are **constants**.
```

In general we would know the values of $a_{ij}$ and $b_i$ and we would like to find out what the values of $x_i$ are. Note: It is possible for some of the coefficients $a_{ij}$ to be zero, in which case not all variables will be visible in all the equations.

Under the right conditions, a system of linear equations will have a single unique solution - one value that each of the variables can take if all the equations are true. For this to happen, we need the number of equations in the system to be the same as the number of unknowns, as well as some conditions on the equations themselves. We will discuss this more in the section on [Consistent, Inconsistent and Indeterminate systems](consistent-inconsistent-and-indeterminate-systems-section).

---

```{index} Systems of linear equations ; matrix equation
```

### Representing systems of linear equations using matrix equations

Systems of linear equations are often represented using a matrix equation $A \mathbf{x} = \mathbf{b}$, where $A$ is an $m \times n$ **coefficient matrix** containing the values of $a_{ij}$, $\mathbf{x}$ is an $m \times 1$ **variable vector** containing the variables $x_i$ and $\mathbf{b}$ is a $m \times 1$ **constant vector** containing the constant terms $b_i$. (Vectors are more formally introduced in the [next chapter](vectors-chapter), but for now simply consider them as matrices with a single column.)

$$ \begin{array}{cccc}
    A & \mathbf{x} & = & \mathbf{b} \\[4pt]
    \underbrace{\begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}}_{\textsf{coefficient matrix}} &
    \underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{\textsf{variable vector}} & = &
    \underbrace{\begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}}_{\textsf{constant vector}}
\end{array} $$

For example, the system of linear equations

$$ \begin{align*}
    2x_1 + x_2 &= 4 \\
    4x_1 + 3x_2 &= 10
\end{align*} $$

can be written as the matrix equation

$$ \begin{align*}
    \begin{pmatrix}
        2 & 1 \\
        4 & 3
    \end{pmatrix}
    \begin{pmatrix}
        x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
        4 \\ 10
    \end{pmatrix}
\end{align*} $$

---

(solving-systems-of-linear-equations-using-algebra-section)=

### Solving systems of linear equations using algebra

One way to solve systems of linear equations is by manipulating them using standard algebraic methods.

The usual approach is to try to **solve for each unknown** in turn by **eliminating all of the other unknowns**. For example, consider the system of linear equations

$$ \begin{align*}
    2x_1 + x_2 &= 4, \\
    4x_1 + 3x_2 &= 10.
\end{align*} $$

If we wish to eliminate the variable $x_2$, we might note that the first equation would still be true if we multiplied both sides by $3$, to get

$$ 6x_1 + 3x_2 = 12 $$

We could then eliminate $x_2$ by subtracting this from the second equation, to give

$$ -2 x_1 = -2,$$

which can be solved to give $x_1 = 1$. Now that we know the value of $x_1$ we can substitute it into any equation in the system to find the value of $x_2$. Substituting $x_1 = 1$ into the first equation gives

$$ 2 \cdot 1 + x_2 = 4, $$

so $x_2 = 2$. The solution should satisfy all equations in the system, so we can also check that these values also satisfy the second equation:

$$ 4x_1 + 3x_2  = 4 \cdot 1 + 3 \cdot 2 = 4 + 6 = 10 $$

So we know that $x_1 = 1$ and $x_2 = 2$ is the solution to this system. While this approach is simple to implement for small systems of linear equations, the algebra will soon become unwieldy for larger systems, which is why we use the methods that are covered in this chapter.

(requirements-for-systems-of-linear-equations-to-be-solvable-section)=

### Requirements for a system to be solvable

Consider this system of two linear equations:

$$ \begin{align*}
    ax + by &= e, \\
    cx + dy &= f.
\end{align*} $$

To solve for $x$, we could look to eliminate $y$ by multiplying the second equation by $\dfrac{b}{d}$ to give

$$ \frac{b}{d}c x + by = \frac{b}{d}f,$$

Subtract this from the first equation

$$ \begin{align*}
    ax - \frac{b}{d}c x &= e - \frac{b}{d}f \\
    (ad - bc)x &= (de - bf) \\
    x &= \frac{de - bf}{ad - bc}.
\end{align*} $$

We can also solve for $y$ by multiplying the first equation by $\dfrac{c}{a}$ to give

$$ c x + \frac{c}{a} b y = \frac{c}{a} e. $$

Subtract this from the second equation

$$ \begin{align*}
    d y - \frac{c}{a} b y &= f - \frac{c}{a} e \\
    (ad - bc) y &= af - ce \\
    y &= \frac{af - ce}{ad - bc}.
\end{align*} $$

The denominators (bottom half of the fractions) in the solutions to $x$ and $y$ are both $a d - b c$, so if this value is zero then the system of equations does not have a solution. If we write the system using matrices:

$$ \begin{align*}
    \begin{pmatrix} a & b \\ c & d \end{pmatrix}
    \begin{pmatrix} x \\ y \end{pmatrix} =
    \begin{pmatrix} e \\ f \end{pmatrix} \quad \implies \quad
    \begin{matrix}
        ax + by \!\!\! &= e, \\
        cx + dy \!\!\! &= f.
    \end{matrix}
\end{align*} $$

We see that the requirement for the system to have a solution is the same as the requirement for matrix to be invertible, i.e, that the value of $ad-bc$ is non-zero. As we will see, inverse matrices are an important tool in solving systems of linear equations.
